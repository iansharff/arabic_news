{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afbe496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "from tools import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeabe510-69c5-4946-976c-ad8cf475fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camel_tools\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.morphology.generator import Generator\n",
    "from camel_tools.morphology.reinflector import Reinflector\n",
    "\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e1e8bb-0a93-4932-8452-8b83cb9fccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = misc.load('raw_data/corpus_df.pkl')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.text, data.cls, random_state=misc.SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07dff6c-719a-4d41-b34f-0b8260852a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Searching...\n",
      "PIPELINE:\n",
      "\tvect\n",
      "\ttfidf\n",
      "\tclf\n",
      "PARAMS:\n",
      "{'clf__alpha': (0.1, 0.5, 1),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0)}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 500.847\n",
      "\n",
      "Best score: 0.960576923076923\n",
      "Best parameters:\n",
      "{'clf': MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True),\n",
      " 'clf__alpha': 0.1,\n",
      " 'clf__class_prior': None,\n",
      " 'clf__fit_prior': True,\n",
      " 'memory': None,\n",
      " 'steps': [('vect',\n",
      "            CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)),\n",
      "           ('tfidf',\n",
      "            TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
      "           ('clf', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
      " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
      " 'tfidf__norm': 'l2',\n",
      " 'tfidf__smooth_idf': True,\n",
      " 'tfidf__sublinear_tf': False,\n",
      " 'tfidf__use_idf': True,\n",
      " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None),\n",
      " 'vect__analyzer': 'word',\n",
      " 'vect__binary': False,\n",
      " 'vect__decode_error': 'strict',\n",
      " 'vect__dtype': <class 'numpy.int64'>,\n",
      " 'vect__encoding': 'utf-8',\n",
      " 'vect__input': 'content',\n",
      " 'vect__lowercase': True,\n",
      " 'vect__max_df': 0.5,\n",
      " 'vect__max_features': None,\n",
      " 'vect__min_df': 1,\n",
      " 'vect__ngram_range': (1, 1),\n",
      " 'vect__preprocessor': None,\n",
      " 'vect__stop_words': None,\n",
      " 'vect__strip_accents': None,\n",
      " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " 'vect__tokenizer': None,\n",
      " 'vect__vocabulary': None,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.1, 0.5, 1)\n",
    "}\n",
    "\n",
    "def main():\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    print(\"Grid Searching...\")\n",
    "    print(\"PIPELINE:\")\n",
    "    print(*[f'\\t{name}' for name, _ in pipeline.steps], sep='\\n')\n",
    "    print(\"PARAMS:\")\n",
    "    pprint(param_grid)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'done in {time() - t0:.3f}')\n",
    "    print()\n",
    "    \n",
    "    print(f'Best score: {grid_search.best_score_}')\n",
    "    print(\"Best parameters:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    pprint(best_parameters)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066b35e7-6527-4ff3-8c8f-f6895a593049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ينظم معهد الشارقة للفنون معرضاً فنياً تحت عنوان باقة الفن، وذلك عند الساعة السابعة من مساء اليوم في مقر المعهد في منطقة الفنون في حي الشويهين في الشارقة، وتتلاقى في المعرض إبداعات 62 طالباً من المنتسبين للدراسة في المعهد في كافة التخصصات الفنية .\n",
      "\n",
      "ينظم معهد الشارقة للفنون معرضاً فنياً تحت عنوان باقة الفن، وذلك عند الساعة السابعة من مساء اليوم في مقر المعهد في منطقة الفنون في حي الشويهين في الشارقة، وتتلاقى في المعرض إبداعات 62 طالباً من المنتسبين للدراسة في المعهد في كافة التخصصات الفنية .\n"
     ]
    }
   ],
   "source": [
    "with open('raw_data/Culture/0000.txt') as f:\n",
    "    text = f.read()\n",
    "## Analyzer  \n",
    "# db = MorphologyDB.builtin_db()\n",
    "# analyzer = Analyzer(db)\n",
    "\n",
    "text_norm = normalize_unicode(text)\n",
    "print(text, text_norm, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85868060-a23d-4764-99db-82efb44619b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نَظَّم,  مَعْهَد,  شارِقَة,  فَنّ,  مَعْرِض,  فَنِّيّ,  عُنْوان,  باقَة,  فَنّ,  ذٰلِكَ,  ساعَة,  سابِع,  مَقَرّ,  مَعْهَد,  مِنْطَقَة,  فَنّ,  حَيّ,  الشويهين,  شارِقَة,  تَلاقَى,  مَعْرِض,  إِبْداع,  طالِب,  مُنْتَسِب,  دِراسَة,  مَعْهَد,  كافَّة,  تَخَصُّص,  فَنِّيّ\n"
     ]
    }
   ],
   "source": [
    "re_tokenizer = RegexpTokenizer(r'[\\u0621-\\u064A]+')\n",
    "re_tokens = re_tokenizer.tokenize(text_norm)\n",
    "\n",
    "tokens_filtered = list(filter(lambda x: (x not in string.punctuation) and (x not in stopwords), re_tokens))\n",
    "\n",
    "mled = MLEDisambiguator.pretrained()\n",
    "disambig = mled.disambiguate(tokens_filtered)\n",
    "\n",
    "final_tokens = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "print(*final_tokens, sep=',  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12f273c-43db-457f-a191-1432a82aa516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text_norm = normalize_unicode(text)\n",
    "    \n",
    "    re_tokenizer = RegexpTokenizer(r'[\\u0621-\\u064A]+')\n",
    "    tokens_re = re_tokenizer.tokenize(text_norm)\n",
    "    \n",
    "    tokens_filtered = list(filter(lambda x: x not in stopwords, tokens_re))\n",
    "        \n",
    "    mled = MLEDisambiguator.pretrained()\n",
    "    disambig = mled.disambiguate(tokens_filtered)\n",
    "    tokens_lem = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    \n",
    "    return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5147f4e5-213c-4f93-96e3-25683b12d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['نَظَّم',\n",
       " 'مَعْهَد',\n",
       " 'شارِقَة',\n",
       " 'فَنّ',\n",
       " 'مَعْرِض',\n",
       " 'فَنِّيّ',\n",
       " 'عُنْوان',\n",
       " 'باقَة',\n",
       " 'فَنّ',\n",
       " 'ذٰلِكَ',\n",
       " 'ساعَة',\n",
       " 'سابِع',\n",
       " 'مَقَرّ',\n",
       " 'مَعْهَد',\n",
       " 'مِنْطَقَة',\n",
       " 'فَنّ',\n",
       " 'حَيّ',\n",
       " 'الشويهين',\n",
       " 'شارِقَة',\n",
       " 'تَلاقَى',\n",
       " 'مَعْرِض',\n",
       " 'إِبْداع',\n",
       " 'طالِب',\n",
       " 'مُنْتَسِب',\n",
       " 'دِراسَة',\n",
       " 'مَعْهَد',\n",
       " 'كافَّة',\n",
       " 'تَخَصُّص',\n",
       " 'فَنِّيّ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e1992-98c9-457e-a323-a0ec0e0aff3c",
   "metadata": {},
   "source": [
    "## Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f9cbf1-c89c-49f2-b806-acde4befbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_data/corpus_df.pkl', 'rb') as f:\n",
    "    corpus_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c908f9-1cba-4334-8d16-db36ad025662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45500 entries, 0 to 45499\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   cls     45500 non-null  object\n",
      " 1   text    45500 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "corpus_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc464c8-9b4f-4703-8458-650a59b2b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = corpus_df.text\n",
    "y = corpus_df.cls\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2520ecfb-13ca-4847-a7d1-433c2b225e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_train.sample(frac=0.01, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4c140d-ecba-49e2-97f4-1db7b900b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(\n",
    "    lowercase=False,\n",
    "    stop_words=stopwords,\n",
    "    max_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34bece90-2a9d-4764-bd7a-c6be4ee4e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iansharff/opt/anaconda3/envs/arabicnews/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:382: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['آمين', 'آها', 'أب', 'أخ', 'أف', 'أفعل', 'أفعله', 'ؤلاء', 'إل', 'إليك', 'إليكن', 'إم', 'إيه', 'ات', 'اتان', 'ارتد', 'انفك', 'بخ', 'برح', 'بس', 'تان', 'تبد', 'تحو', 'تعل', 'حد', 'حم', 'حي', 'خب', 'ذار', 'ذان', 'سيما', 'شتان', 'صه', 'ظن', 'عد', 'قط', 'كأي', 'مر', 'مكان', 'مكانكن', 'نب', 'هات', 'هاك', 'هب', 'واها', 'وراء'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "X_train_cv = countvec.fit_transform(X_train)\n",
    "X_test_cv = countvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bf555e9-83fa-49d9-bd00-8c2a742924d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>تشير</th>\n",
       "      <th>خبراء</th>\n",
       "      <th>برامج</th>\n",
       "      <th>الحماية</th>\n",
       "      <th>أجهزة</th>\n",
       "      <th>الكمبيوتر</th>\n",
       "      <th>الإمارات</th>\n",
       "      <th>ارتفاع</th>\n",
       "      <th>معدلات</th>\n",
       "      <th>الهجمات</th>\n",
       "      <th>...</th>\n",
       "      <th>بلاتيني</th>\n",
       "      <th>التطعيم</th>\n",
       "      <th>توام</th>\n",
       "      <th>مشغلي</th>\n",
       "      <th>المصاحبة</th>\n",
       "      <th>كردستان</th>\n",
       "      <th>النمل</th>\n",
       "      <th>دارفور</th>\n",
       "      <th>بوسكي</th>\n",
       "      <th>حديثي</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36400 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       تشير  خبراء  برامج  الحماية  أجهزة  الكمبيوتر  الإمارات  ارتفاع  \\\n",
       "0         0      0      0        0      0          0         0       0   \n",
       "1         0      0      0        0      0          0         0       0   \n",
       "2         0      0      0        0      0          0         0       0   \n",
       "3         0      0      0        0      0          0         0       0   \n",
       "4         0      0      0        0      0          0         0       0   \n",
       "...     ...    ...    ...      ...    ...        ...       ...     ...   \n",
       "36395     0      0      0        0      0          0         0       0   \n",
       "36396     0      0      0        0      0          0         0       0   \n",
       "36397     0      0      0        0      0          0         0       0   \n",
       "36398     0      0      0        0      0          0         0       0   \n",
       "36399     0      0      0        0      0          0         0       0   \n",
       "\n",
       "       معدلات  الهجمات  ...  بلاتيني  التطعيم  توام  مشغلي  المصاحبة  كردستان  \\\n",
       "0           0        0  ...        0        0     0      0         0        0   \n",
       "1           0        0  ...        0        0     0      0         0        0   \n",
       "2           0        0  ...        0        0     0      0         0        0   \n",
       "3           0        0  ...        0        0     0      0         0        0   \n",
       "4           0        0  ...        0        0     0      0         0        1   \n",
       "...       ...      ...  ...      ...      ...   ...    ...       ...      ...   \n",
       "36395       0        0  ...        0        0     1      0         0        0   \n",
       "36396       0        0  ...        0        0     0      0         0        0   \n",
       "36397       0        0  ...        0        0     0      0         0        0   \n",
       "36398       0        0  ...        0        0     0      0         0        0   \n",
       "36399       0        0  ...        0        0     0      0         0        0   \n",
       "\n",
       "       النمل  دارفور  بوسكي  حديثي  \n",
       "0          0       0      0      0  \n",
       "1          0       0      0      0  \n",
       "2          0       0      0      0  \n",
       "3          0       0      0      0  \n",
       "4          0       0      0      0  \n",
       "...      ...     ...    ...    ...  \n",
       "36395      0       0      0      0  \n",
       "36396      0       0      0      0  \n",
       "36397      0       0      0      0  \n",
       "36398      0       0      0      0  \n",
       "36399      0       0      0      0  \n",
       "\n",
       "[36400 rows x 10000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_cv.todense(), columns=countvec.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arabicnews",
   "language": "python",
   "name": "arabicnews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "c781e7e7c556a0a96987dc067ce9407646832babf618d3d3c4d5f966ec834848"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
